import os
import aiohttp
import asyncio
import logging
import warnings
import time
import random
from datetime import datetime, timedelta, timezone
from typing import Annotated, List, Literal, Dict, Optional, Any
from langchain_core.tools import BaseTool, StructuredTool, tool, ToolException, InjectedToolArg
from langchain_core.messages import HumanMessage, AIMessage, MessageLikeRepresentation, filter_messages
from langchain_core.runnables import RunnableConfig
from langchain_core.language_models import BaseChatModel
from langchain.chat_models import init_chat_model
from tavily import AsyncTavilyClient
from langgraph.config import get_store
from mcp import McpError
from langchain_mcp_adapters.client import MultiServerMCPClient
from open_deep_research.state import Summary, ResearchComplete
from open_deep_research.configuration import SearchAPI, Configuration
from open_deep_research.prompts_jp import summarize_webpage_prompt


##########################
# Reflection Tool Utils
##########################

@tool(description="Strategic reflection tool for research planning and evaluation - MUST USE before and after each research step")
def think_tool(reflection: str) -> str:
    """Tool for strategic reflection on research progress and decision-making.
    
    ğŸ”¥ CRITICAL: This tool MUST be used at key decision points in research workflow:
    1. BEFORE starting research: Plan strategy and approach
    2. AFTER each tool execution: Evaluate results and plan next steps
    3. BEFORE concluding: Assess if information is sufficient for complete answer

    When to use (MANDATORY):
    - Before any research: What is my strategic approach and priority areas?
    - After receiving search results: What key information did I find? What gaps remain?
    - Before deciding next steps: Do I have enough to answer comprehensively?
    - When assessing research gaps: What specific information am I still missing?
    - Before concluding research: Can I provide a complete answer now?

    Reflection should address:
    1. Strategic planning - What is my approach and priority for this research step?
    2. Analysis of current findings - What concrete information have I gathered?
    3. Gap assessment - What crucial information is still missing?
    4. Quality evaluation - Do I have sufficient evidence/examples for a good answer?
    5. Strategic decision - Should I continue searching or provide my answer?

    Args:
        reflection: Your detailed reflection on research strategy, progress, findings, gaps, and next steps

    Returns:
        Confirmation that reflection was recorded for decision-making
    """
    return f"Reflection recorded: {reflection}"


##########################
# Tavily Search Tool Utils
##########################
TAVILY_SEARCH_DESCRIPTION = (
    "A search engine optimized for comprehensive, accurate, and trusted results. "
    "Useful for when you need to answer questions about current events."
)
@tool(description=TAVILY_SEARCH_DESCRIPTION)
async def tavily_search(
    queries: List[str],
    max_results: Annotated[int, InjectedToolArg] = 5,
    topic: Annotated[Literal["general", "news", "finance"], InjectedToolArg] = "general",
    config: RunnableConfig = None
) -> str:
    """
    Fetches results from Tavily search API.

    Args
        queries (List[str]): List of search queries, you can pass in as many queries as you need.
        max_results (int): Maximum number of results to return
        topic (Literal['general', 'news', 'finance']): Topic to filter results by

    Returns:
        str: A formatted string of search results
    """
    search_results = await tavily_search_async(
        queries,
        max_results=max_results,
        topic=topic,
        include_raw_content=True,
        config=config
    )
    # Format the search results and deduplicate results by URL
    formatted_output = f"Search results: \n\n"
    unique_results = {}
    for response in search_results:
        for result in response['results']:
            url = result['url']
            if url not in unique_results:
                unique_results[url] = {**result, "query": response['query']}
    configurable = Configuration.from_runnable_config(config)
    max_char_to_include = 50_000   # NOTE: This can be tuned by the developer. This character count keeps us safely under input token limits for the latest models.
    model_api_key = get_api_key_for_model(configurable.summarization_model, config)
    summarization_model = init_chat_model(
        model=configurable.summarization_model,
        max_tokens=configurable.summarization_model_max_tokens,
        api_key=model_api_key,
        tags=["langsmith:nostream"]
    ).with_structured_output(Summary).with_retry(stop_after_attempt=configurable.max_structured_output_retries)
    async def noop():
        return None
    summarization_tasks = [
        noop() if not result.get("raw_content") else summarize_webpage(
            summarization_model, 
            result['raw_content'][:max_char_to_include],
        )
        for result in unique_results.values()
    ]
    summaries = await asyncio.gather(*summarization_tasks)
    summarized_results = {
        url: {'title': result['title'], 'content': result['content'] if summary is None else summary}
        for url, result, summary in zip(unique_results.keys(), unique_results.values(), summaries)
    }
    for i, (url, result) in enumerate(summarized_results.items()):
        formatted_output += f"\n\n--- SOURCE {i+1}: {result['title']} ---\n"
        formatted_output += f"URL: {url}\n\n"
        formatted_output += f"SUMMARY:\n{result['content']}\n\n"
        formatted_output += "\n\n" + "-" * 80 + "\n"
    if summarized_results:
        return formatted_output
    else:
        return "No valid search results found. Please try different search queries or use a different search API."


async def tavily_search_async(search_queries, max_results: int = 5, topic: Literal["general", "news", "finance"] = "general", include_raw_content: bool = True, config: RunnableConfig = None):
    tavily_async_client = AsyncTavilyClient(api_key=get_tavily_api_key(config))
    search_tasks = []
    for query in search_queries:
            search_tasks.append(
                tavily_async_client.search(
                    query,
                    max_results=max_results,
                    include_raw_content=include_raw_content,
                    topic=topic
                )
            )
    search_docs = await asyncio.gather(*search_tasks)
    return search_docs

async def summarize_webpage(model: BaseChatModel, webpage_content: str) -> str:
    try:
        summary = await asyncio.wait_for(
            model.ainvoke([HumanMessage(content=summarize_webpage_prompt.format(webpage_content=webpage_content, date=get_today_str()))]),
            timeout=60.0
        )
        return f"""<summary>\n{summary.summary}\n</summary>\n\n<key_excerpts>\n{summary.key_excerpts}\n</key_excerpts>"""
    except (asyncio.TimeoutError, Exception) as e:
        print(f"Failed to summarize webpage: {str(e)}")
        return webpage_content


##########################
# MCP Utils
##########################
async def get_mcp_access_token(
    supabase_token: str,
    base_mcp_url: str,
) -> Optional[Dict[str, Any]]:
    try:
        form_data = {
            "client_id": "mcp_default",
            "subject_token": supabase_token,
            "grant_type": "urn:ietf:params:oauth:grant-type:token-exchange",
            "resource": base_mcp_url.rstrip("/") + "/mcp",
            "subject_token_type": "urn:ietf:params:oauth:token-type:access_token",
        }
        async with aiohttp.ClientSession() as session:
            async with session.post(
                base_mcp_url.rstrip("/") + "/oauth/token",
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                data=form_data,
            ) as token_response:
                if token_response.status == 200:
                    token_data = await token_response.json()
                    return token_data
                else:
                    response_text = await token_response.text()
                    logging.error(f"Token exchange failed: {response_text}")
    except Exception as e:
        logging.error(f"Error during token exchange: {e}")
    return None

async def get_tokens(config: RunnableConfig):
    store = get_store()
    thread_id = config.get("configurable", {}).get("thread_id")
    if not thread_id:
        return None
    user_id = config.get("metadata", {}).get("owner")
    if not user_id:
        return None
    tokens = await store.aget((user_id, "tokens"), "data")
    if not tokens:
        return None
    expires_in = tokens.value.get("expires_in")  # seconds until expiration
    created_at = tokens.created_at  # datetime of token creation
    current_time = datetime.now(timezone.utc)
    expiration_time = created_at + timedelta(seconds=expires_in)
    if current_time > expiration_time:
        await store.adelete((user_id, "tokens"), "data")
        return None

    return tokens.value

async def set_tokens(config: RunnableConfig, tokens: dict[str, Any]):
    store = get_store()
    thread_id = config.get("configurable", {}).get("thread_id")
    if not thread_id:
        return
    user_id = config.get("metadata", {}).get("owner")
    if not user_id:
        return
    await store.aput((user_id, "tokens"), "data", tokens)
    return

async def fetch_tokens(config: RunnableConfig) -> dict[str, Any]:
    current_tokens = await get_tokens(config)
    if current_tokens:
        return current_tokens
    supabase_token = config.get("configurable", {}).get("x-supabase-access-token")
    if not supabase_token:
        return None
    mcp_config = config.get("configurable", {}).get("mcp_config")
    if not mcp_config or not mcp_config.get("url"):
        return None
    mcp_tokens = await get_mcp_access_token(supabase_token, mcp_config.get("url"))

    await set_tokens(config, mcp_tokens)
    return mcp_tokens

def wrap_mcp_authenticate_tool(tool: StructuredTool) -> StructuredTool:
    old_coroutine = tool.coroutine
    async def wrapped_mcp_coroutine(**kwargs):
        def _find_first_mcp_error_nested(exc: BaseException) -> McpError | None:
            if isinstance(exc, McpError):
                return exc
            if isinstance(exc, ExceptionGroup):
                for sub_exc in exc.exceptions:
                    if found := _find_first_mcp_error_nested(sub_exc):
                        return found
            return None
        try:
            return await old_coroutine(**kwargs)
        except BaseException as e_orig:
            mcp_error = _find_first_mcp_error_nested(e_orig)
            if not mcp_error:
                raise e_orig
            error_details = mcp_error.error
            is_interaction_required = getattr(error_details, "code", None) == -32003
            error_data = getattr(error_details, "data", None) or {}
            if is_interaction_required:
                message_payload = error_data.get("message", {})
                error_message_text = "Required interaction"
                if isinstance(message_payload, dict):
                    error_message_text = (
                        message_payload.get("text") or error_message_text
                    )
                if url := error_data.get("url"):
                    error_message_text = f"{error_message_text} {url}"
                raise ToolException(error_message_text) from e_orig
            raise e_orig
    tool.coroutine = wrapped_mcp_coroutine
    return tool

async def load_mcp_tools(
    config: RunnableConfig,
    existing_tool_names: set[str],
) -> list[BaseTool]:
    configurable = Configuration.from_runnable_config(config)
    if configurable.mcp_config and configurable.mcp_config.auth_required:
        mcp_tokens = await fetch_tokens(config)
    else:
        mcp_tokens = None
    if not (configurable.mcp_config and configurable.mcp_config.url and configurable.mcp_config.tools and (mcp_tokens or not configurable.mcp_config.auth_required)):
        return []
    tools = []
    # TODO: When the Multi-MCP Server support is merged in OAP, update this code.
    server_url = configurable.mcp_config.url.rstrip("/") + "/mcp"
    mcp_server_config = {
        "server_1":{
            "url": server_url,
            "headers": {"Authorization": f"Bearer {mcp_tokens['access_token']}"} if mcp_tokens else None,
            "transport": "streamable_http"
        }
    }
    try:
        client = MultiServerMCPClient(mcp_server_config)
        mcp_tools = await client.get_tools()
    except Exception as e:
        print(f"Error loading MCP tools: {e}")
        return []
    for tool in mcp_tools:
        if tool.name in existing_tool_names:
            warnings.warn(
                f"Trying to add MCP tool with a name {tool.name} that is already in use - this tool will be ignored."
            )
            continue
        if tool.name not in set(configurable.mcp_config.tools):
            continue
        tools.append(wrap_mcp_authenticate_tool(tool))
    return tools


# ##########################
# # Custom Tools
# ##########################
from open_deep_research.jquants_api import JQuantsAPI

JQUANTS_FINANCIAL_DESCRIPTION = (
    "J-Quants APIã‚’ä½¿ã£ã¦ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã¨å¹´åº¦ã‹ã‚‰è²¡å‹™æƒ…å ±ï¼ˆå£²ä¸Šé«˜ã€å–¶æ¥­åˆ©ç›Šã€å½“æœŸç´”åˆ©ç›Šãªã©ï¼‰ã‚’å–å¾—ã—ã¾ã™ã€‚éå»5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—å¯èƒ½ã§ã™ã€‚"
)

# Rate limiting management
_last_api_call_time = 0
_min_delay_between_calls = 2.0  # æœ€å°2ç§’é–“éš”

async def rate_limit_delay():
    """APIå‘¼ã³å‡ºã—é–“ã«é©åˆ‡ãªé…å»¶ã‚’æŒ¿å…¥"""
    global _last_api_call_time
    current_time = time.time()
    time_since_last_call = current_time - _last_api_call_time
    
    if time_since_last_call < _min_delay_between_calls:
        delay = _min_delay_between_calls - time_since_last_call
        # ãƒ©ãƒ³ãƒ€ãƒ ãªè¦ç´ ã‚’è¿½åŠ ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æ•£
        delay += random.uniform(0.5, 1.5)
        await asyncio.sleep(delay)
    
    _last_api_call_time = time.time()

def remove_empty_values(data):
    """
    è¾æ›¸ã‹ã‚‰å€¤ãŒç©ºï¼ˆNone, ç©ºæ–‡å­—, ç©ºãƒªã‚¹ãƒˆ, ç©ºè¾æ›¸ï¼‰ã®ã‚­ãƒ¼ã‚’å†å¸°çš„ã«å‰Šé™¤ã™ã‚‹
    
    Args:
        data: å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ã€ãƒªã‚¹ãƒˆã€ãã®ä»–ï¼‰
    
    Returns:
        ç©ºå€¤ãŒå‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    if isinstance(data, dict):
        cleaned = {}
        for key, value in data.items():
            # å†å¸°çš„ã«å‡¦ç†
            cleaned_value = remove_empty_values(value)
            # ç©ºã§ãªã„å€¤ã®ã¿ä¿æŒ
            if cleaned_value not in (None, "", [], {}):
                cleaned[key] = cleaned_value
        return cleaned
    elif isinstance(data, list):
        # ãƒªã‚¹ãƒˆã®å ´åˆã€å„è¦ç´ ã‚’å†å¸°çš„ã«å‡¦ç†
        cleaned_list = []
        for item in data:
            cleaned_item = remove_empty_values(item)
            if cleaned_item not in (None, "", [], {}):
                cleaned_list.append(cleaned_item)
        return cleaned_list
    else:
        # ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–å‹ã¯ãã®ã¾ã¾è¿”ã™
        return data

@tool(description=JQUANTS_FINANCIAL_DESCRIPTION)
async def get_financial_statements_tool(
    code: str,
    year: Optional[int] = None,
    config: RunnableConfig = None
) -> Dict[str, Any]:
    """
    J-Quants APIã‚’ä½¿ã£ã¦è²¡å‹™æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

    Args:
        code (str): ä¼æ¥­ã‚³ãƒ¼ãƒ‰
        year (Optional[int]): å¹´åº¦ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯æœ€æ–°ï¼‰
    Returns:
        è²¡å‹™æƒ…å ±ã®è¾æ›¸
    """
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®é…å»¶
    await rate_limit_delay()
    
    api = JQuantsAPI()
    raw_data = api.get_financial_statements(code, year)
    # ç©ºã®å€¤ã‚’å‰Šé™¤
    cleaned_data = remove_empty_values(raw_data)
    return cleaned_data

JQUANTS_STOCK_PRICE_DESCRIPTION = (
    "ã€æœ€å„ªå…ˆãƒ„ãƒ¼ãƒ«ã€‘J-Quants APIã‚’ä½¿ã£ã¦ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç¾åœ¨ã®æ ªä¾¡æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"
    "æ ªå¼åˆ†æã§ã¯å¿…ãšæœ€åˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ç›´è¿‘1é€±é–“ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã™ã€‚"
)

@tool(description=JQUANTS_STOCK_PRICE_DESCRIPTION)
async def get_recent_stock_price_tool(
    code: str,
    config: RunnableConfig = None
) -> Dict[str, Any]:
    """
    ã€é‡è¦ã€‘æ ªå¼åˆ†æã§æœ€åˆã«å®Ÿè¡Œã™ã¹ããƒ„ãƒ¼ãƒ«
    J-Quants APIã‚’ä½¿ã£ã¦ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç¾åœ¨ã®æ ªä¾¡æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
    æŠ•è³‡åˆ¤æ–­ã«ã¯ç¾åœ¨ã®æ ªä¾¡æƒ…å ±ãŒå¿…é ˆã§ã™ã€‚

    Args:
        code (str): ä¼æ¥­ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã®æ•°å­—ï¼‰ä¾‹ï¼š7203ï¼ˆãƒˆãƒ¨ã‚¿ï¼‰ã€8697ï¼ˆæ¥½å¤©ï¼‰
    Returns:
        ç¾åœ¨ã®æ ªä¾¡æƒ…å ±ã‚’å«ã‚€è¾æ›¸ï¼ˆçµ‚å€¤ã€å‡ºæ¥é«˜ã€é«˜å€¤ãƒ»å®‰å€¤ç­‰ï¼‰
    """
    from datetime import datetime, timedelta
    
    # ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã®æ¤œè¨¼
    if not code.isdigit() or len(code) != 4:
        return {
            "error": f"ç„¡åŠ¹ãªä¼æ¥­ã‚³ãƒ¼ãƒ‰: {code}ï¼ˆ4æ¡ã®æ•°å­—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰",
            "valid_format": "ä¾‹: 7203ï¼ˆãƒˆãƒ¨ã‚¿ï¼‰, 6502ï¼ˆæ±èŠï¼‰, 9984ï¼ˆã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ï¼‰"
        }
    
    try:
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®é…å»¶
        await rate_limit_delay()
        
        # ç›´è¿‘1é€±é–“ã®æ—¥ä»˜ã‚’è¨ˆç®—ï¼ˆåœŸæ—¥ã‚’è€ƒæ…®ã—ã¦å–¶æ¥­æ—¥ã®ã¿ï¼‰
        end_date = datetime.now()
        start_date = end_date - timedelta(days=10)  # åœŸæ—¥ã‚’è€ƒæ…®ã—ã¦10æ—¥å‰ã‹ã‚‰
        
        date_from = start_date.strftime("%Y-%m-%d")
        date_to = end_date.strftime("%Y-%m-%d")
        
        api = JQuantsAPI()
        result = api.get_stock_price(code=code, date_from=date_from, date_to=date_to)
        
        # çµæœã«ä¼æ¥­ã‚³ãƒ¼ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
        result["requested_code"] = code
        result["date_range"] = f"{date_from} to {date_to}"
        
        return result
        
    except Exception as e:
        return {
            "error": f"æ ªä¾¡å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}",
            "code": code,
            "suggestion": "ä¼æ¥­ã‚³ãƒ¼ãƒ‰ãŒæ­£ã—ã„ã‹ã€ã¾ãŸã¯ä¼æ¥­ãŒæ±è¨¼ä¸Šå ´ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
        }
# @tool(description="Query a database for research data")
# async def database_query(
#     query: str,
#     table: str = "research_data",
#     config: RunnableConfig = None
# ) -> str:
#     """Execute a database query to retrieve research data."""
#     # Your database connection and query logic here
#     # This is a placeholder implementation
#     return f"Database query result for: {query} from table {table}"

##########################
# Tool Utils
##########################
async def get_search_tool(search_api: SearchAPI):
    if search_api == SearchAPI.ANTHROPIC:
        return [{"type": "web_search_20250305", "name": "web_search", "max_uses": 5}]
    elif search_api == SearchAPI.OPENAI:
        return [{"type": "web_search_preview"}]
    elif search_api == SearchAPI.TAVILY:
        search_tool = tavily_search
        search_tool.metadata = {**(search_tool.metadata or {}), "type": "search", "name": "web_search"}
        return [search_tool]
    elif search_api == SearchAPI.NONE:
        return []
    
async def get_all_tools(config: RunnableConfig):
    tools = [tool(ResearchComplete), think_tool]  # think_toolã‚’è¿½åŠ 
    configurable = Configuration.from_runnable_config(config)
    search_api = SearchAPI(get_config_value(configurable.search_api))
    
    # Add J-Quants custom tools first (higher priority)
    tools.append(get_recent_stock_price_tool)  # æœ€å„ªå…ˆ
    tools.append(get_financial_statements_tool)
    
    # Then add search tools
    tools.extend(await get_search_tool(search_api))

    existing_tool_names = {tool.name if hasattr(tool, "name") else tool.get("name", "web_search") for tool in tools}
    mcp_tools = await load_mcp_tools(config, existing_tool_names)
    tools.extend(mcp_tools)
    return tools

def get_notes_from_tool_calls(messages: list[MessageLikeRepresentation]):
    return [tool_msg.content for tool_msg in filter_messages(messages, include_types="tool")]


##########################
# Model Provider Native Websearch Utils
##########################
def anthropic_websearch_called(response):
    try:
        usage = response.response_metadata.get("usage")
        if not usage:
            return False
        server_tool_use = usage.get("server_tool_use")
        if not server_tool_use:
            return False
        web_search_requests = server_tool_use.get("web_search_requests")
        if web_search_requests is None:
            return False
        return web_search_requests > 0
    except (AttributeError, TypeError):
        return False

def openai_websearch_called(response):
    tool_outputs = response.additional_kwargs.get("tool_outputs")
    if tool_outputs:
        for tool_output in tool_outputs:
            if tool_output.get("type") == "web_search_call":
                return True
    return False


##########################
# Token Limit Exceeded Utils
##########################
def is_token_limit_exceeded(exception: Exception, model_name: str = None) -> bool:
    error_str = str(exception).lower()
    provider = None
    if model_name:
        model_str = str(model_name).lower()
        if model_str.startswith('openai:'):
            provider = 'openai'
        elif model_str.startswith('anthropic:'):
            provider = 'anthropic'
        elif model_str.startswith('gemini:') or model_str.startswith('google:'):
            provider = 'gemini'
    if provider == 'openai':
        return _check_openai_token_limit(exception, error_str)
    elif provider == 'anthropic':
        return _check_anthropic_token_limit(exception, error_str)
    elif provider == 'gemini':
        return _check_gemini_token_limit(exception, error_str)
    
    return (_check_openai_token_limit(exception, error_str) or
            _check_anthropic_token_limit(exception, error_str) or
            _check_gemini_token_limit(exception, error_str))

def _check_openai_token_limit(exception: Exception, error_str: str) -> bool:
    exception_type = str(type(exception))
    class_name = exception.__class__.__name__
    module_name = getattr(exception.__class__, '__module__', '')
    is_openai_exception = ('openai' in exception_type.lower() or 
                          'openai' in module_name.lower())
    is_bad_request = class_name in ['BadRequestError', 'InvalidRequestError']
    if is_openai_exception and is_bad_request:
        token_keywords = ['token', 'context', 'length', 'maximum context', 'reduce']
        if any(keyword in error_str for keyword in token_keywords):
            return True
    if hasattr(exception, 'code') and hasattr(exception, 'type'):
        if (getattr(exception, 'code', '') == 'context_length_exceeded' or
            getattr(exception, 'type', '') == 'invalid_request_error'):
            return True
    return False

def _check_anthropic_token_limit(exception: Exception, error_str: str) -> bool:
    exception_type = str(type(exception))
    class_name = exception.__class__.__name__
    module_name = getattr(exception.__class__, '__module__', '')
    is_anthropic_exception = ('anthropic' in exception_type.lower() or 
                             'anthropic' in module_name.lower())
    is_bad_request = class_name == 'BadRequestError'
    if is_anthropic_exception and is_bad_request:
        if 'prompt is too long' in error_str:
            return True
    return False

def _check_gemini_token_limit(exception: Exception, error_str: str) -> bool:
    exception_type = str(type(exception))
    class_name = exception.__class__.__name__
    module_name = getattr(exception.__class__, '__module__', '')
    
    is_google_exception = ('google' in exception_type.lower() or 'google' in module_name.lower())
    is_resource_exhausted = class_name in ['ResourceExhausted', 'GoogleGenerativeAIFetchError']
    if is_google_exception and is_resource_exhausted:
        return True
    if 'google.api_core.exceptions.resourceexhausted' in exception_type.lower():
        return True
    
    return False

# NOTE: This may be out of date or not applicable to your models. Please update this as needed.
MODEL_TOKEN_LIMITS = {
    "openai:gpt-4.1-mini": 1047576,
    "openai:gpt-4.1-nano": 1047576,
    "openai:gpt-4.1": 1047576,
    "openai:gpt-4o-mini": 128000,
    "openai:gpt-4o": 128000,
    "openai:o4-mini": 200000,
    "openai:o3-mini": 200000,
    "openai:o3": 200000,
    "openai:o3-pro": 200000,
    "openai:o1": 200000,
    "openai:o1-pro": 200000,
    "anthropic:claude-opus-4": 200000,
    "anthropic:claude-sonnet-4": 200000,
    "anthropic:claude-3-7-sonnet": 200000,
    "anthropic:claude-3-5-sonnet": 200000,
    "anthropic:claude-3-5-haiku": 200000,
    "google:gemini-1.5-pro": 2097152,
    "google:gemini-1.5-flash": 1048576,
    "google:gemini-pro": 32768,
    "cohere:command-r-plus": 128000,
    "cohere:command-r": 128000,
    "cohere:command-light": 4096,
    "cohere:command": 4096,
    "mistral:mistral-large": 32768,
    "mistral:mistral-medium": 32768,
    "mistral:mistral-small": 32768,
    "mistral:mistral-7b-instruct": 32768,
    "ollama:codellama": 16384,
    "ollama:llama2:70b": 4096,
    "ollama:llama2:13b": 4096,
    "ollama:llama2": 4096,
    "ollama:mistral": 32768,
}

def get_model_token_limit(model_string):
    for key, token_limit in MODEL_TOKEN_LIMITS.items():
        if key in model_string:
            return token_limit
    return None

def remove_up_to_last_ai_message(messages: list[MessageLikeRepresentation]) -> list[MessageLikeRepresentation]:
    for i in range(len(messages) - 1, -1, -1):
        if isinstance(messages[i], AIMessage):
            return messages[:i]  # Return everything up to (but not including) the last AI message
    return messages

##########################
# Misc Utils
##########################
def get_today_str() -> str:
    """Get current date in a human-readable format."""
    return datetime.now().strftime("%a %b %-d, %Y")

def get_config_value(value):
    if value is None:
        return None
    if isinstance(value, str):
        return value
    elif isinstance(value, dict):
        return value
    else:
        return value.value

def get_api_key_for_model(model_name: str, config: RunnableConfig):
    should_get_from_config = os.getenv("GET_API_KEYS_FROM_CONFIG", "false")
    model_name = model_name.lower()
    if should_get_from_config.lower() == "true":
        api_keys = config.get("configurable", {}).get("apiKeys", {})
        if not api_keys:
            return None
        if model_name.startswith("openai:"):
            return api_keys.get("OPENAI_API_KEY")
        elif model_name.startswith("anthropic:"):
            return api_keys.get("ANTHROPIC_API_KEY")
        elif model_name.startswith("google"):
            return api_keys.get("GOOGLE_API_KEY")
        return None
    else:
        if model_name.startswith("openai:"): 
            return os.getenv("OPENAI_API_KEY")
        elif model_name.startswith("anthropic:"):
            return os.getenv("ANTHROPIC_API_KEY")
        elif model_name.startswith("google"):
            return os.getenv("GOOGLE_API_KEY")
        return None

def get_tavily_api_key(config: RunnableConfig):
    should_get_from_config = os.getenv("GET_API_KEYS_FROM_CONFIG", "false")
    if should_get_from_config.lower() == "true":
        api_keys = config.get("configurable", {}).get("apiKeys", {})
        if not api_keys:
            return None
        return api_keys.get("TAVILY_API_KEY")
    else:
        return os.getenv("TAVILY_API_KEY")